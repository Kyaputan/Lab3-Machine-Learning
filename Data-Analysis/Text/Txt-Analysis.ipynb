{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR ANALYSIS/CLEANING/COMPUTATION:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#FOR VISUALIZATION:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Loading data file now, this could take a while depending on file size\")\n",
    "start = time.time()\n",
    "df = pd.read_csv('DATASET.csv') # ADD-CSV\n",
    "end = time.time()\n",
    "print(\"Loading took \" + str(round(end - start, 2)) + \" seconds\\n\")\n",
    "print(\"Number of rows : \",df.shape[0],\" and the number of columns : \",df.shape[1])\n",
    "missing_values = df.isna().sum().sum()\n",
    "duplicated_values = df.duplicated().sum()\n",
    "print(f'\\nMissing values: {missing_values}')\n",
    "print(f'Duplicated values: {duplicated_values}')\n",
    "if missing_values >= 1:\n",
    "    print('\\nMissing values by column:')\n",
    "    print(df.isna().sum())\n",
    "print(\"\\nUnique Values in Each Column:\")\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_distributions(columns, data=df, palette='muted'):\n",
    "    plt.figure(figsize=(15, 6))  \n",
    "    \n",
    "    for i, column_name in enumerate(columns):\n",
    "        plt.subplot(1, 3, i + 1)  \n",
    "        value_counts = data[column_name].value_counts()\n",
    "        value_counts.plot.pie(autopct='%1.1f%%', colors=sns.color_palette\n",
    "                            (palette), startangle=90, explode=[0.05] * value_counts.nunique())\n",
    "        \n",
    "        plt.title(f'Percentage Distribution of {column_name}')\n",
    "        plt.ylabel('')  \n",
    "    df[columns].value_counts()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "columns_to_plot = ['label']\n",
    "plot_categorical_distributions(columns_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    text = re.sub(r'Ã[\\x80-\\xBF]+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Zก-ฮะ-์\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_text'] = df['Review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df.iloc[0]['Review'])\n",
    "print(df.iloc[0]['Clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ทำ TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)  \n",
    "    return tokens  \n",
    "df['Tokens'] = df['Clean_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dict = {'2morrow': 'tomorrow', '2nite': 'tonight', '4ever': 'forever', '4get': 'forget', '4give': 'forgive', '4got': 'forgot', '4th': 'fourth', '4ward': 'forward', '4warned': 'forewarned', '4wrd': 'forward', 'abt': 'about', 'acc': 'account', 'acct': 'account', 'add': 'address', 'addy': 'address', \n",
    "             'admin': 'administrator', 'advert': 'advertise', 'advice': 'advise', 'aftr': 'after', 'agri': 'agriculture', 'aint': 'am not', 'alot': 'a lot', 'alrite': 'all right', 'alryt': 'all right', 'alwys': 'always', 'amblnc': 'ambulance', 'amnt': 'amount', 'amp': 'amplifier', 'aniversary': 'anniversary', 'anniv': 'anniversary'}\n",
    "\n",
    "def normalize_text(text):\n",
    "    for word in text.split():\n",
    "        if word in norm_dict:\n",
    "            text = text.replace(word, norm_dict[word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(tokens):\n",
    "\treturn [norm_dict.get(token, token) for token in tokens]\n",
    "\n",
    "df['normalize'] = df['Tokens'].apply(normalize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = {'app', 'music','play', 'spotify', 'song', 'songs', 'listen', 'playing','get', 'playlist'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))  # Get the set of English stopwords\n",
    "    if custom_stopwords:\n",
    "        stop_words.update(custom_stopwords)\n",
    "    return [word for word in tokens if word.lower() not in stop_words]  # Filter out stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### เลือก Custom Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = {'app', 'music','play', 'spotify', 'song', 'songs', 'listen', 'playing','get', 'playlist'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Filtered_Tokens'] = df['Tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "วนลบ Stop Words โดยใช้ Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "all_filtered_tokens = [word for tokens in df['Filtered_Tokens'] for word in tokens]\n",
    "\n",
    "word_counts_after_removal = Counter(all_filtered_tokens)\n",
    "most_common_words_after_removal = word_counts_after_removal.most_common(20)\n",
    "\n",
    "print(most_common_words_after_removal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# แก้ LEMMATIZATION พวก V-ing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "#pos is set to v (verb) for better accuracy. v เพื่อจาก running -> run , a เพื่อจาก better -> good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemmatized_Tokens'] = df['Filtered_Tokens'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# การทำเป็น Input ให้โมเดล\n",
    "### แบ่งได้ 3 วิธี \n",
    "1. ทำ TF-IDF\n",
    "2. ทำ One-Hot Encoding\n",
    "3. ทำ Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ใช้ sklearn text ทำ Tfidf_matrix และ Feature_names\n",
    "ง่ายเร็ว คำมีความเกี่ยวข้องกัน ดี กับ ดีมาก มีค่าใกล้กัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def apply_tfidf(df , max_features=10000):\n",
    "    df['TFIDF_Tokens'] = df['Lemmatized_Tokens'].apply(lambda x: ' '.join(x))\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=max_features , encoding='utf-8', decode_error='replace',lowercase=False)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['TFIDF_Tokens'])\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    return tfidf_matrix, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix, feature_names = apply_tfidf(df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 : <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 10 stored elements and shape (1, 10000)>\n",
      "  Coords\tValues\n",
      "  (0, 3121)\t0.19645623338718804\n",
      "  (0, 8265)\t0.2699783522857679\n",
      "  (0, 519)\t0.3252689617050176\n",
      "  (0, 3551)\t0.38189959165106974\n",
      "  (0, 7620)\t0.28049351556628804\n",
      "  (0, 2116)\t0.2644618482301777\n",
      "  (0, 9543)\t0.17685364605762688\n",
      "  (0, 7632)\t0.4456317394459237\n",
      "  (0, 2700)\t0.38212337351102066\n",
      "  (0, 8860)\t0.3352157943109041\n",
      "Review 1 : <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 7 stored elements and shape (1, 10000)>\n",
      "  Coords\tValues\n",
      "  (0, 3121)\t0.20186968304578454\n",
      "  (0, 3871)\t0.49057624581594506\n",
      "  (0, 7463)\t0.39818900683462105\n",
      "  (0, 6723)\t0.48138994952960623\n",
      "  (0, 7682)\t0.3694340736085449\n",
      "  (0, 8852)\t0.34968223570709517\n",
      "  (0, 8698)\t0.2636922614032367\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision = 2)\n",
    "for i , tfide_enc in enumerate(tfidf_matrix[:2]):\n",
    "    print(f\"Review {i+1} : {tfide_enc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_wordcloud(df,sentence_column, emotion_column, condition_value):\n",
    "    \"\"\"\n",
    "    สร้าง WordCloud จากข้อมูลใน DataFrame\n",
    "    Parameters:\n",
    "        df (DataFrame): ข้อมูลในรูปแบบ pandas DataFrame\n",
    "        sentence_column (str): ชื่อคอลัมน์ที่มีข้อความ\n",
    "        emotion_column (str): ชื่อคอลัมน์สำหรับการกรองข้อมูล\n",
    "        condition_value (str): ค่าของเงื่อนไขที่ใช้กรอง\n",
    "    \"\"\"\n",
    "    filtered_text = \" \".join(df[df[emotion_column] == condition_value][sentence_column])\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        collocations=True,\n",
    "        max_words=2000\n",
    "    ).generate(filtered_text)\n",
    "    \n",
    "    # แสดงผล WordCloud\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # ซ่อนแกน x และ y\n",
    "    plt.title(f\"WordCloud for {emotion_column} = {condition_value}\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def plot_emotion_wordclouds(df,sentence_column ,emotion_column , figsize=(15, 6)):\n",
    "    \"\"\"\n",
    "    สร้าง Word Cloud สำหรับแต่ละอารมณ์ใน DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame ที่มีข้อมูลอารมณ์และข้อความ\n",
    "        emotion_column (str): ชื่อคอลัมน์ที่แสดงอารมณ์\n",
    "        sentence_column (str): ชื่อคอลัมน์ที่แสดงข้อความ\n",
    "        figsize (tuple): ขนาดของกราฟ (default: (15, 6))\n",
    "        background_color (str): สีพื้นหลังของ Word Cloud (default: 'white')\n",
    "        colormap (str): ชุดสีของ Word Cloud (default: 'Reds')\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # ดึงรายการอารมณ์ที่ไม่ซ้ำกันและเรียงลำดับ\n",
    "    unique_emotions = sorted(df[emotion_column].unique())\n",
    "    \n",
    "    for emotion in unique_emotions:\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # รวมข้อความทั้งหมดของอารมณ์นั้น\n",
    "        emotion_review = \" \".join(df[df[emotion_column] == emotion][sentence_column])\n",
    "        \n",
    "        stopwords = set(STOPWORDS)\n",
    "        # สร้าง Word Cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color='white',\n",
    "            stopwords=stopwords,\n",
    "            collocations=True,\n",
    "            max_words=2000\n",
    "        ).generate(emotion_review)\n",
    "        \n",
    "        # แสดง Word Cloud\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"WordCloud for {emotion_column} = {emotion}\", fontsize=16)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(df, sentence_column='TFIDF_Tokens', emotion_column='label', condition_value='POSITIVE')\n",
    "generate_wordcloud(df, sentence_column='TFIDF_Tokens', emotion_column='label', condition_value='NEGATIVE')\n",
    "plot_emotion_wordclouds(df,sentence_column ='TFIDF_Tokens',emotion_column ='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X = tfidf_matrix  # Features from TF-IDF\n",
    "y = df['label']   # Target labels\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "real_accuracy= accuracy*100\n",
    "print(f\"Logistic Regression Accuracy: {real_accuracy:.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred_class = model.predict(X_test)\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_class))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_class))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['NEGATIVE','POSITIVE'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix for sentiment Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encodeing\n",
    "ง่ายเร็วแต่คำทุกคำจะไม่เกี่ยวข้องหรือสือถึงกันได้ เช่น ดี กับ ดีมาก model จะมองว่า 2 คำนี้ต่างกันแบบสิ้นเชิง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df['Lemmatized_Tokens'].tolist()\n",
    "\n",
    "# แสดงผลลัพธ์\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(word for row in result for word in row if word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(unique_words)[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word = len(unique_words)\n",
    "w2ids = {w: idx for idx, w in enumerate(unique_words)}\n",
    "print(w2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def One_Hot_Encode(x , n_class):\n",
    "    return np.eye(n_class)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for w, index in w2ids.items():\n",
    "    print(f\"{w:<10}\", '\\t', One_Hot_Encode(index, total_word))\n",
    "    count += 1\n",
    "    if count >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encodings = []\n",
    "tokens_ids = [w2ids[tk] for tk in result[0]]\n",
    "tokens_encode = [One_Hot_Encode(id, total_word) for id in tokens_ids]\n",
    "\n",
    "print(f\"{'word':<5}\\t{'id':<5}\\t{'encoding':<100}\")\n",
    "for tk, id, en in zip(result[0], tokens_ids, tokens_encode):\n",
    "    print(f\"{tk:<5}\\t{id:<5}\\t{str(en):<100}\")\n",
    "\n",
    "print(result[0], ':', sum(tokens_encode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({'text': result[0:5]})\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(new_df.text.apply(pd.Series).stack()).groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ใช้ sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer\n",
    "เหมือน One-Encode แต่จะเพิ่มความถี่ของคำเข้าไปได้ คำไม่เกี่ยวข้องกันเหมือนเดิม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df['Lemmatized_Tokens'].tolist()\n",
    "texts = [\"cat cat fish\", \"cat fish\", \"fish bird\", \"bird\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tokens back into strings\n",
    "joined_result = [' '.join(tokens) for tokens in result]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "CV_fit = cv.fit_transform(texts)\n",
    "word_list = cv.get_feature_names_out()\n",
    "count_list =  np.asarray(CV_fit.sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird': np.int64(2), 'cat': np.int64(3), 'fish': np.int64(3)}\n"
     ]
    }
   ],
   "source": [
    "print(dict(zip(word_list, count_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_fit.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "ไม่มีใครใช้แล้ว"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
