{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\",\n",
    "          \"B_ORG\", \"B_PER\", \n",
    "          \"B_LOC\", \"B_MEA\", \n",
    "          \"I_DTM\", \"I_ORG\", \n",
    "          \"E_ORG\", \"I_PER\", \n",
    "          \"B_TTL\", \"E_PER\", \n",
    "          \"B_DES\", \"E_LOC\", \n",
    "          \"B_DTM\", \"B_NUM\", \n",
    "          \"I_MEA\", \"E_DTM\", \n",
    "          \"E_MEA\", \"I_LOC\", \n",
    "          \"I_DES\", \"E_DES\", \n",
    "          \"I_NUM\", \"E_NUM\", \n",
    "          \"B_TRM\", \"B_BRN\", \n",
    "          \"I_TRM\", \"E_TRM\", \n",
    "          \"I_TTL\", \"I_BRN\", \n",
    "          \"E_BRN\", \"E_TTL\", \"B_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def make_csv_for_text(input_folder, output_csv):\n",
    "    with open(output_csv, mode=\"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Filename\", \"words\", \"POS\", \"Entity\", \"Class\"])\n",
    "    \n",
    "        for filename in sorted(os.listdir(input_folder)):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(input_folder, filename)\n",
    "    \n",
    "                with open(file_path, mode=\"r\", encoding=\"utf-8\") as txt_file:\n",
    "                    for line in txt_file:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if len(parts) > 1:\n",
    "                            writer.writerow([filename] + parts)\n",
    "    \n",
    "    print(f\"‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå '{input_folder}' ‡πÄ‡∏õ‡πá‡∏ô '{output_csv}' ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå 'train/train' ‡πÄ‡∏õ‡πá‡∏ô 'train.csv' ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå 'eval/eval' ‡πÄ‡∏õ‡πá‡∏ô 'eval.csv' ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå 'test/test' ‡πÄ‡∏õ‡πá‡∏ô 'test.csv' ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n"
     ]
    }
   ],
   "source": [
    "make_csv_for_text(input_folder=\"train/train\",output_csv=\"train.csv\")\n",
    "make_csv_for_text(input_folder=\"eval/eval\",output_csv=\"eval.csv\")\n",
    "make_csv_for_text(input_folder=\"test/test\",output_csv=\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file now, this could take a while depending on file size\n",
      "Loading took 1.64 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Loading data file now, this could take a while depending on file size\")\n",
    "start = time.time()\n",
    "df_train = pd.read_csv('train.csv',index_col=False)\n",
    "df_eval =  pd.read_csv('eval.csv',index_col=False)\n",
    "df_test =  pd.read_csv('test.csv',index_col=False)\n",
    "end = time.time()\n",
    "print(\"Loading took \" + str(round(end - start, 2)) + \" seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>words</th>\n",
       "      <th>POS</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡∏™‡∏†‡∏≤‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏™‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®</td>\n",
       "      <td>NN</td>\n",
       "      <td>B_ORG</td>\n",
       "      <td>B_CLS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡πÑ‡∏ó‡∏¢</td>\n",
       "      <td>NN</td>\n",
       "      <td>E_ORG</td>\n",
       "      <td>I_CLS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡∏à‡∏µ‡πâ</td>\n",
       "      <td>VV</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡∏®‡∏≤‡∏•</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡πÑ‡∏ü‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß</td>\n",
       "      <td>VV</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename                        words POS Entity  Class\n",
       "0  00001.txt  ‡∏™‡∏†‡∏≤‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏™‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®  NN  B_ORG  B_CLS\n",
       "1  00001.txt                          ‡πÑ‡∏ó‡∏¢  NN  E_ORG  I_CLS\n",
       "2  00001.txt                          ‡∏à‡∏µ‡πâ  VV      O  I_CLS\n",
       "3  00001.txt                          ‡∏®‡∏≤‡∏•  NN      O  I_CLS\n",
       "4  00001.txt                      ‡πÑ‡∏ü‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß  VV      O  I_CLS"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'O': 0, 'B_ORG': 1, 'B_PER': 2, 'B_LOC': 3, 'B_MEA': 4, 'I_DTM': 5, 'I_ORG': 6, 'E_ORG': 7, 'I_PER': 8, 'B_TTL': 9, 'E_PER': 10, 'B_DES': 11, 'E_LOC': 12, 'B_DTM': 13, 'B_NUM': 14, 'I_MEA': 15, 'E_DTM': 16, 'E_MEA': 17, 'I_LOC': 18, 'I_DES': 19, 'E_DES': 20, 'I_NUM': 21, 'E_NUM': 22, 'B_TRM': 23, 'B_BRN': 24, 'I_TRM': 25, 'E_TRM': 26, 'I_TTL': 27, 'I_BRN': 28, 'E_BRN': 29, 'E_TTL': 30, 'B_NAME': 31}\n",
      "id2label: {0: 'O', 1: 'B_ORG', 2: 'B_PER', 3: 'B_LOC', 4: 'B_MEA', 5: 'I_DTM', 6: 'I_ORG', 7: 'E_ORG', 8: 'I_PER', 9: 'B_TTL', 10: 'E_PER', 11: 'B_DES', 12: 'E_LOC', 13: 'B_DTM', 14: 'B_NUM', 15: 'I_MEA', 16: 'E_DTM', 17: 'E_MEA', 18: 'I_LOC', 19: 'I_DES', 20: 'E_DES', 21: 'I_NUM', 22: 'E_NUM', 23: 'B_TRM', 24: 'B_BRN', 25: 'I_TRM', 26: 'E_TRM', 27: 'I_TTL', 28: 'I_BRN', 29: 'E_BRN', 30: 'E_TTL', 31: 'B_NAME'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tag list from the CSV file\n",
    "tag_list_df = pd.read_csv('tag_list.csv')\n",
    "\n",
    "# Create label2id and id2label dictionaries\n",
    "label2id = {label: idx for idx, label in enumerate(tag_list_df['tag'])}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "num_labels=len(label2id)\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Entity'] = df_train['Entity'].apply(lambda x: x if x in labels else \"O\")\n",
    "df_eval['Entity'] = df_eval['Entity'].apply(lambda x: x if x in labels else \"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ner'] = df_train['Entity'].map(label2id)\n",
    "df_eval['ner'] = df_eval['Entity'].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id_pos_dict: {'NN': 0, 'VV': 1, 'PU': 2, 'CC': 3, 'PS': 4, 'NG': 5, 'AX': 6, 'NU': 7, 'AV': 8, 'AJ': 9, 'FX': 10, 'CL': 11, 'PR': 12, 'PA': 13, 'XX': 14, 'IJ': 15}\n",
      "\n",
      "\n",
      "id2label_pos_dict: {0: 'NN', 1: 'VV', 2: 'PU', 3: 'CC', 4: 'PS', 5: 'NG', 6: 'AX', 7: 'NU', 8: 'AV', 9: 'AJ', 10: 'FX', 11: 'CL', 12: 'PR', 13: 'PA', 14: 'XX', 15: 'IJ'}\n"
     ]
    }
   ],
   "source": [
    "label2id_pos_dict = {label: idx for idx, label in enumerate(df_train['POS'].unique())}\n",
    "id2label_pos_dict = {idx: label for label, idx in label2id_pos_dict.items()}\n",
    "# Step 2: Map the POS column using the created dictionary\n",
    "\n",
    "df_train['pos_tags'] = df_train['POS'].map(label2id_pos_dict)\n",
    "df_eval['pos_tags'] = df_eval['POS'].map(label2id_pos_dict)\n",
    "df_test['pos_tags'] = df_test['POS'].map(label2id_pos_dict)\n",
    "\n",
    "print(\"label2id_pos_dict:\", label2id_pos_dict)\n",
    "print(\"\\n\")\n",
    "print(\"id2label_pos_dict:\", id2label_pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>words</th>\n",
       "      <th>POS</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Class</th>\n",
       "      <th>ner</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡∏™‡∏†‡∏≤‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏™‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®</td>\n",
       "      <td>NN</td>\n",
       "      <td>B_ORG</td>\n",
       "      <td>B_CLS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡πÑ‡∏ó‡∏¢</td>\n",
       "      <td>NN</td>\n",
       "      <td>E_ORG</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡∏à‡∏µ‡πâ</td>\n",
       "      <td>VV</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡∏®‡∏≤‡∏•</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00001.txt</td>\n",
       "      <td>‡πÑ‡∏ü‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß</td>\n",
       "      <td>VV</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804216</th>\n",
       "      <td>03794.txt</td>\n",
       "      <td>‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á</td>\n",
       "      <td>PS</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804217</th>\n",
       "      <td>03794.txt</td>\n",
       "      <td>‡πÄ‡∏£‡∏µ‡∏¢‡∏ô</td>\n",
       "      <td>VV</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804218</th>\n",
       "      <td>03794.txt</td>\n",
       "      <td>‡∏Å‡∏±‡∏ö</td>\n",
       "      <td>PS</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804219</th>\n",
       "      <td>03794.txt</td>\n",
       "      <td>‡∏á‡∏≤‡∏ô</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804220</th>\n",
       "      <td>03794.txt</td>\n",
       "      <td>\"</td>\n",
       "      <td>PU</td>\n",
       "      <td>O</td>\n",
       "      <td>E_CLS</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2804221 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Filename                        words POS Entity  Class  ner  \\\n",
       "0        00001.txt  ‡∏™‡∏†‡∏≤‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏™‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®  NN  B_ORG  B_CLS    1   \n",
       "1        00001.txt                          ‡πÑ‡∏ó‡∏¢  NN  E_ORG  I_CLS    7   \n",
       "2        00001.txt                          ‡∏à‡∏µ‡πâ  VV      O  I_CLS    0   \n",
       "3        00001.txt                          ‡∏®‡∏≤‡∏•  NN      O  I_CLS    0   \n",
       "4        00001.txt                      ‡πÑ‡∏ü‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß  VV      O  I_CLS    0   \n",
       "...            ...                          ...  ..    ...    ...  ...   \n",
       "2804216  03794.txt                      ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á  PS      O  I_CLS    0   \n",
       "2804217  03794.txt                        ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô  VV      O  I_CLS    0   \n",
       "2804218  03794.txt                          ‡∏Å‡∏±‡∏ö  PS      O  I_CLS    0   \n",
       "2804219  03794.txt                          ‡∏á‡∏≤‡∏ô  NN      O  I_CLS    0   \n",
       "2804220  03794.txt                            \"  PU      O  E_CLS    0   \n",
       "\n",
       "         pos_tags  \n",
       "0               0  \n",
       "1               0  \n",
       "2               1  \n",
       "3               0  \n",
       "4               1  \n",
       "...           ...  \n",
       "2804216         4  \n",
       "2804217         1  \n",
       "2804218         4  \n",
       "2804219         0  \n",
       "2804220         2  \n",
       "\n",
       "[2804221 rows x 7 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>words</th>\n",
       "      <th>POS</th>\n",
       "      <th>Class</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03795.txt</td>\n",
       "      <td>‡∏£‡∏±‡∏ê</td>\n",
       "      <td>NN</td>\n",
       "      <td>B_CLS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03795.txt</td>\n",
       "      <td>‡∏ñ‡∏±‡∏á‡πÅ‡∏ï‡∏Å</td>\n",
       "      <td>VV</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03795.txt</td>\n",
       "      <td>‡∏ß‡∏¥‡∏Å</td>\n",
       "      <td>NN</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03795.txt</td>\n",
       "      <td>_</td>\n",
       "      <td>NN</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03795.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>NN</td>\n",
       "      <td>I_CLS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename   words POS  Class  pos_tags\n",
       "0  03795.txt     ‡∏£‡∏±‡∏ê  NN  B_CLS         0\n",
       "1  03795.txt  ‡∏ñ‡∏±‡∏á‡πÅ‡∏ï‡∏Å  VV  I_CLS         1\n",
       "2  03795.txt     ‡∏ß‡∏¥‡∏Å  NN  I_CLS         0\n",
       "3  03795.txt       _  NN  I_CLS         0\n",
       "4  03795.txt       7  NN  I_CLS         0"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Class'] = df_test[\"Entity\"]\n",
    "df_test.drop(columns=[\"Entity\"], inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.rename(columns={'Filename':'sentence_id'}, inplace=True)\n",
    "df_eval.rename(columns={'Filename':'sentence_id'}, inplace=True)\n",
    "df_test.rename(columns={'Filename':'sentence_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_id_sentence(df):\n",
    "    new_sentence_ids = []\n",
    "    sentence_counter = -1\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Class'] == 'B_CLS':\n",
    "            sentence_counter += 1\n",
    "        new_sentence_ids.append(sentence_counter)\n",
    "    df['sentence_id'] = new_sentence_ids\n",
    "new_id_sentence(df_train)\n",
    "new_id_sentence(df_eval)\n",
    "new_id_sentence(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=[\"Class\"], inplace=True)\n",
    "df_eval.drop(columns=[\"Class\"], inplace=True)\n",
    "df_test.drop(columns=[\"Class\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder_ner = LabelEncoder()\n",
    "label_encoder_pos = LabelEncoder()\n",
    "\n",
    "df_train['encoded_ner'] = label_encoder_ner.fit_transform(df_train['ner'])\n",
    "df_train['encoded_pos'] = label_encoder_pos.fit_transform(df_train['pos_tags'])\n",
    "\n",
    "df_eval['encoded_ner'] = label_encoder_ner.fit_transform(df_eval['ner'])\n",
    "df_eval['encoded_pos'] = label_encoder_pos.fit_transform(df_eval['pos_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['pos_tags'])\n",
    "\n",
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "df_train = df_train.rename(columns={\n",
    "    'words': 'word',\n",
    "    'Entity': 'entity',\n",
    "    'ner': 'label'\n",
    "})\n",
    "\n",
    "label_encoder_ner = LabelEncoder()\n",
    "df_train['encoded_labels'] = label_encoder_ner.fit_transform(df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_eval.drop(columns=['pos_tags'])\n",
    "\n",
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "df_eval = df_eval.rename(columns={\n",
    "    'words': 'word',\n",
    "    'Entity': 'entity',\n",
    "    'ner': 'label'\n",
    "})\n",
    "label_encoder_ner = LabelEncoder()\n",
    "df_eval['encoded_labels'] = label_encoder_ner.fit_transform(df_eval['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['‡∏™‡∏†‡∏≤‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏™‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®', '‡πÑ‡∏ó‡∏¢', '‡∏à‡∏µ‡πâ', '‡∏®‡∏≤‡∏•', '‡πÑ‡∏ü‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß', '‡∏Ç‡∏≤‡∏¢', '‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå', '‡∏õ‡∏π', '‡∏ó‡∏≤‡∏á', '‡∏õ‡∏£‡∏±‡∏ö']\n",
      "Labels: [1, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Entities: ['B_ORG', 'E_ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "words = df_train['word'].tolist()  # list ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥\n",
    "labels = df_train['encoded_labels'].tolist()  # list ‡∏Ç‡∏≠‡∏á label (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)\n",
    "entities = df_train['entity'].tolist()  # list ‡∏Ç‡∏≠‡∏á entity (‡πÄ‡∏ä‡πà‡∏ô B_ORG, O)\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "print(\"Words:\", words[:10])\n",
    "print(\"Labels:\", labels[:10])\n",
    "print(\"Entities:\", entities[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['sentence_id', 'POS', 'encoded_pos', 'encoded_labels'])\n",
    "df_eval = df_eval.drop(columns=['sentence_id', 'POS', 'encoded_pos', 'encoded_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "tokenizer = BertTokenizer.from_pretrained('Geotrend/bert-base-th-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['word'], \n",
    "        truncation=True,  # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
    "        padding='max_length',  # ‡πÄ‡∏ï‡∏¥‡∏° padding ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "        max_length=512,  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = examples['encoded_ner']\n",
    "    new_labels = []\n",
    "    \n",
    "    for word, label in zip(examples['word'], labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        label_for_word = [label] * len(tokenized_word)  # ‡∏ó‡∏∏‡∏Å token ‡∏à‡∏∞‡∏°‡∏µ label ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
    "        \n",
    "        new_labels.extend(label_for_word)\n",
    "\n",
    "    new_labels = new_labels[:512] + [-100] * (512 - len(new_labels))  # Padding ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß 512\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Dataset ‡∏Ç‡∏≠‡∏á Hugging Face\n",
    "Train = Dataset.from_pandas(df_train)\n",
    "Eval = Dataset.from_pandas(df_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/248470 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 4 named input_ids expected length 1000 but got length 512",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[380], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Eval_tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mEval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m Train_tokenized_datasets \u001b[38;5;241m=\u001b[39m Train\u001b[38;5;241m.\u001b[39mmap(tokenize_and_align_labels, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\datasets\\arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3056\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3057\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\datasets\\arrow_dataset.py:3481\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3479\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3481\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3482\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\datasets\\arrow_writer.py:608\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    606\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    607\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m--> 608\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\pyarrow\\table.pxi:4868\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\pyarrow\\table.pxi:4214\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 4 named input_ids expected length 1000 but got length 512"
     ]
    }
   ],
   "source": [
    "Eval_tokenized_datasets = Eval.map(tokenize_and_align_labels, batched=True)\n",
    "Train_tokenized_datasets = Train.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics (accuracy, precision, recall, F1 score)\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)  # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å logits ‡πÄ‡∏õ‡πá‡∏ô class ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "    \n",
    "    # ‡πÉ‡∏ä‡πâ precision_recall_fscore_support ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Precision, Recall, F1, Accuracy\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• NER\n",
    "model = BertForTokenClassification.from_pretrained('Geotrend/bert-base-th-cased', num_labels=32)\n",
    "\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=5,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=Train_tokenized_datasets,\n",
    "    eval_dataset=Eval_tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Geotrend/bert-base-th-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification , DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
    "model = AutoModelForTokenClassification.from_pretrained(pretrained, num_labels=len(label_list),ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"BertForToken\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_64648\\1061095081.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_Train,\n",
    "    eval_dataset=tokenized_datasets_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1752640 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\NLP\\Lib\\site-packages\\transformers\\trainer.py:3654\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m-> 3654\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3656\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3657\u001b[0m         )\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
